{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_level_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUCALdnUjtyb",
        "colab_type": "text"
      },
      "source": [
        "# Character-level CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tvl1Rn5jlhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " %tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn0yVItwjjdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import ceil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import shutil\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvgLJVzRlUT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from IPython.display import clear_output\n",
        "import collections\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El8DW22AaxbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f12R-J8MZ2kE",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxpkEAJac40V",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-wHS-iRZu1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_classes(data_path, sep='\\t'):\n",
        "    return len(pd.read_csv(data_path, header=None, usecols=[0], sep=sep)[0].unique())\n",
        "\n",
        "# Dataset should be tab separated; left hand column is classes starting with 0\n",
        "def create_dataset(data_path, alphabet=\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\",\n",
        "                   max_length=1014, batch_size=128, is_training=True, sep='\\t'):\n",
        "\n",
        "    # Load data with pandas\n",
        "    data = pd.read_csv(data_path, header=None, sep=sep)\n",
        "    num_iters = ceil(data.shape[0] / batch_size)\n",
        "\n",
        "    # Shuffle if it's the training set\n",
        "    if is_training:\n",
        "        data = shuffle(data, random_state=42)\n",
        "    num_columns = data.shape[1]\n",
        "\n",
        "    # If there are more than two columns, add them to col 1 and drop rest\n",
        "    for idx in range(2, num_columns):\n",
        "        data[1] += ' ' + data[idx]\n",
        "    data = data.drop([idx for idx in range(2, num_columns)], axis=1).values\n",
        "    alphabet = list(alphabet)\n",
        "    identity_mat = np.identity(len(alphabet))\n",
        "\n",
        "    # Returns matrix of one-hot column vectors for each row, and a class label integer\n",
        "    def generator():\n",
        "        for row in data:\n",
        "            label, text = row\n",
        "            text = np.array([identity_mat[alphabet.index(i)] for i in list(str(text)) if i in alphabet], dtype=np.float32)\n",
        "            if len(text) > max_length:\n",
        "                text = text[:max_length]\n",
        "            elif 0 < len(text) < max_length:\n",
        "                text = np.concatenate((text, np.zeros((max_length - len(text), len(alphabet)), dtype=np.float32)))\n",
        "            elif len(text) == 0:\n",
        "                text = np.zeros((max_length, len(alphabet)), dtype=np.float32)\n",
        "            yield text.T, label\n",
        "\n",
        "    # Return dataset containing generator, and number of iterations\n",
        "    # Here we specify output types and shapes\n",
        "    return tf.data.Dataset.from_generator(generator, (tf.float32, tf.int32),\n",
        "                                          ((len(alphabet), max_length), (None))).batch(batch_size), num_iters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pjHbrBOk2Lw",
        "colab_type": "text"
      },
      "source": [
        "## Plotting Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AxLzw-Vk0sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot that updates each time it's fed new data\n",
        "# Accepts a dictionary of lists; keys will be used as legend\n",
        "# Update the dictionary, and call again to replot\n",
        "def live_plot(data_dict, figsize=(7,5), title='', xlabel=''):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=figsize)\n",
        "    for label,data in data_dict.items():\n",
        "        plt.plot(data, label=label)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.legend(loc='center left') # the plot evolves to the right\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOit8NISlwnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To test uncomment the following:\n",
        "\n",
        "# fizz = collections.defaultdict(list)\n",
        "# for i in range(10):\n",
        "#     fizz['foo'].append(np.random.random())\n",
        "#     fizz['bar'].append(np.random.random())\n",
        "#     fizz['baz'].append(np.random.random())\n",
        "#     live_plot(fizz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MumujGY6nawk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same as above, but two side-by-side plots\n",
        "def live_plot_double(dict_A, dict_B, xlabel_A='iteration', xlabel_B='epoch', figsize=(13,5)):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    plt.figure(figsize=(13,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    #plt.figure(figsize=(7,5))\n",
        "    for label,data in dict_A.items():\n",
        "        plt.plot(data, label=label)\n",
        "    #plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.xlabel(xlabel_A)\n",
        "    plt.legend(loc='center left') # the plot evolves to the right\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    #plt.figure(figsize=(7,5))\n",
        "    for label,data in dict_B.items():\n",
        "        plt.plot(data, label=label)\n",
        "    #plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.xlabel(xlabel_B)\n",
        "    plt.legend(loc='center left') # the plot evolves to the right\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EtehnIJnVOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fizz = collections.defaultdict(list)\n",
        "# buzz = collections.defaultdict(list)\n",
        "# for i in range(10):\n",
        "#     fizz['foo'].append(np.random.random())\n",
        "#     fizz['bar'].append(np.random.random())\n",
        "#     buzz['baz'].append(np.random.random())\n",
        "#     buzz['foobar'].append(np.random.random())\n",
        "#     live_plot_double(fizz, buzz)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPYiradQdp_n",
        "colab_type": "text"
      },
      "source": [
        "# The Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCBrc1ZfZGGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Char_level_cnn(object):\n",
        "    def __init__(self, batch_size=128, num_classes=14, feature=\"small\", \n",
        "                 kernel_size=[7, 7, 3, 3, 3, 3], padding=\"VALID\"):\n",
        "        super(Char_level_cnn, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        if feature == \"small\":\n",
        "            self.num_filters = 256\n",
        "            self.stddev_initialization = 0.05\n",
        "            self.num_fully_connected_features = 1024\n",
        "        else:\n",
        "            self.num_filters = 1024\n",
        "            self.stddev_initialization = 0.02\n",
        "            self.num_fully_connected_features = 2048\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, input, keep_prob):\n",
        "\n",
        "        output = tf.expand_dims(input, -1)\n",
        "        output = self._create_conv(output, [output.get_shape().as_list()[1], self.kernel_size[0], 1, self.num_filters],\n",
        "                                   \"conv1\",\n",
        "                                   3)\n",
        "        output = self._create_conv(output, [1, self.kernel_size[1], self.num_filters, self.num_filters], \"conv2\", 3)\n",
        "        output = self._create_conv(output, [1, self.kernel_size[2], self.num_filters, self.num_filters], \"conv3\")\n",
        "        output = self._create_conv(output, [1, self.kernel_size[3], self.num_filters, self.num_filters], \"conv4\")\n",
        "        output = self._create_conv(output, [1, self.kernel_size[4], self.num_filters, self.num_filters], \"conv5\")\n",
        "        output = self._create_conv(output, [1, self.kernel_size[5], self.num_filters, self.num_filters], \"conv6\", 3)\n",
        "\n",
        "        new_feature_size = int(self.num_filters * ((input.get_shape().as_list()[2] - 96) / 27))\n",
        "        flatten = tf.reshape(output, [-1, new_feature_size])\n",
        "\n",
        "        output = self._create_fc(flatten, [new_feature_size, self.num_fully_connected_features], \"fc1\", keep_prob)\n",
        "        output = self._create_fc(output, [self.num_fully_connected_features, self.num_fully_connected_features], \"fc2\",\n",
        "                                 keep_prob)\n",
        "        output = self._create_fc(output, [self.num_fully_connected_features, self.num_classes], \"fc3\")\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _create_conv(self, input, shape, name_scope, pool_size=None):\n",
        "        with tf.name_scope(name_scope):\n",
        "            weight = self._initialize_weight(shape, self.stddev_initialization)\n",
        "            bias = self._initialize_bias([shape[-1]])\n",
        "            conv = tf.nn.conv2d(input=input, filter=weight, strides=[1, 1, 1, 1], padding=self.padding, name='conv')\n",
        "            activation = tf.nn.relu(tf.nn.bias_add(conv, bias), name=\"relu\")\n",
        "            if pool_size:\n",
        "                return tf.nn.max_pool(value=activation, ksize=[1, 1, pool_size, 1], strides=[1, 1, pool_size, 1],\n",
        "                                      padding=self.padding, name='maxpool')\n",
        "            else:\n",
        "                return activation\n",
        "\n",
        "    def _create_fc(self, input, shape, name_scope, keep_prob=None):\n",
        "        with tf.name_scope(name_scope):\n",
        "            weight = self._initialize_weight(shape, self.stddev_initialization)\n",
        "            bias = self._initialize_bias([shape[-1]])\n",
        "            dense = tf.nn.bias_add(tf.matmul(input, weight), bias, name=\"dense\")\n",
        "            if keep_prob is not None:\n",
        "                return tf.nn.dropout(dense, keep_prob, name=\"dropout\")\n",
        "            else:\n",
        "                return dense\n",
        "\n",
        "    def _initialize_weight(self, shape, stddev):\n",
        "        return tf.Variable(tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32, name='weight'))\n",
        "\n",
        "    def _initialize_bias(self, shape):\n",
        "        return tf.Variable(tf.constant(0, shape=shape, dtype=tf.float32, name='bias'))\n",
        "\n",
        "    def loss(self, logits, labels):\n",
        "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "\n",
        "    def accuracy(self, logits, labels):\n",
        "        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(labels, tf.int64)), dtype=tf.float32))\n",
        "\n",
        "    def confusion_matrix(self, logits, labels):\n",
        "        return tf.confusion_matrix(tf.cast(labels, tf.int64), tf.argmax(logits, 1), num_classes=self.num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCDfi5CIfpEn",
        "colab_type": "text"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7dvczg9iWLk",
        "colab_type": "text"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNOPejxKfqxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training and test files should be tab separated, with integer class in left-hand column,\n",
        "# with first class index of 0\n",
        "# (0, 1, 2, 3...)\n",
        "def train(training_path='', test_path='', save_path='', alphabet=\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\", \n",
        "          batch_size=32, feature='small', num_epochs=20, test_interval=1, plot_interval=20, \n",
        "          lr=1e-2, optimizer='adam', dropout=0.5, es_min_delta=0, es_patience=3,\n",
        "          random_state=None):\n",
        "    \"\"\"\n",
        "    alphabet, Valid characters used for model, e.g. \"GATC\" for DNA classification\n",
        "    train_set, \"data/train.csv\", \"Path to the training set\"\n",
        "    test_set, \"data/test.csv\", \"Path to the test set\"\n",
        "    test_interval, 1, \"Number of epochs between testing phases\"\n",
        "    plot_interval, 20, \"Number of iterations between updating loss plot\"\n",
        "    feature, \"small\", \"large or small\"\n",
        "    batch_size, 128, \"Minibatch size\"\n",
        "    num_epochs, 20, \"Number of training epochs\"\n",
        "    lr, 1e-2, \"Learning rate\"\n",
        "    optimizer, \"sgd\", \"sgd or adam\"\n",
        "    dropout, 0.5, \"Dropout's probability\"\n",
        "    save_path, \"trained_models\", \"path to store trained model\"\n",
        "    es_min_delta, 0., \"Early stopping's parameter: minimum change loss to qualify as an improvement\"\n",
        "    es_patience\", 3, \"Early stopping's parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique\")\n",
        "    \"\"\"\n",
        "  \n",
        "    # Don't change this unless you are sure it will work with network architecture\n",
        "    max_length=1014 \n",
        "\n",
        "    allow_soft_placement=True\n",
        "    log_device_placement=False\n",
        "\n",
        "    # Get number of classes\n",
        "    num_classes = get_num_classes(training_path)\n",
        "    print('Number of classes detected: %d' % (num_classes))\n",
        "    model = Char_level_cnn(batch_size=batch_size, num_classes=num_classes, feature=feature)\n",
        "\n",
        "    # Save loss history\n",
        "    iteration_history = collections.defaultdict(list)\n",
        "    epoch_history = collections.defaultdict(list)\n",
        "    epoch_history['train_loss']\n",
        "    epoch_history['test_loss']\n",
        "    epoch_history['test_accuracy']\n",
        "\n",
        "    # Initialize the operation graph\n",
        "    with tf.Graph().as_default():\n",
        "        session_conf = tf.ConfigProto(\n",
        "            allow_soft_placement=allow_soft_placement,\n",
        "            log_device_placement=log_device_placement)\n",
        "        session_conf.gpu_options.allow_growth = True\n",
        "\n",
        "        # Set graph random seed\n",
        "        if random_state != None:\n",
        "            tf.set_random_seed(random_state)\n",
        "\n",
        "        # Generate dataset and dataset iterator\n",
        "        training_set, num_training_iters = create_dataset(training_path, alphabet, max_length,\n",
        "                                                          batch_size, True)\n",
        "        test_set, num_test_iters = create_dataset(test_path, alphabet, max_length, batch_size, False)\n",
        "        train_iterator = training_set.make_initializable_iterator()\n",
        "        test_iterator = test_set.make_initializable_iterator()\n",
        "\n",
        "        handle = tf.placeholder(tf.string, shape=[])\n",
        "        keep_prob = tf.placeholder(tf.float32, name='dropout_prob')\n",
        "\n",
        "        iterator = tf.data.Iterator.from_string_handle(handle, training_set.output_types, training_set.output_shapes)\n",
        "        texts, labels = iterator.get_next()\n",
        "\n",
        "        # Model functions\n",
        "        logits = model.forward(texts, keep_prob)\n",
        "        loss = model.loss(logits, labels)\n",
        "        accuracy = model.accuracy(logits, labels)\n",
        "        batch_size = tf.unstack(tf.shape(texts))[0]\n",
        "        confusion = model.confusion_matrix(logits, labels)\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "\n",
        "        if optimizer == \"sgd\":\n",
        "            values = [lr]\n",
        "            boundaries = []\n",
        "            for i in range(1, 10):\n",
        "                values.append(lr / pow(2, i))\n",
        "                boundaries.append(3 * num_training_iters * i)\n",
        "            learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
        "            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "        else:\n",
        "            optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        # Save model parameters to disk\n",
        "        if os.path.isdir(save_path):\n",
        "            shutil.rmtree(save_path)\n",
        "        os.makedirs(save_path)\n",
        "        output_file = open(save_path + os.sep + \"logs.txt\", \"w\")\n",
        "        training_parameters = {'alphabet': alphabet, \n",
        "          'batch_size': batch_size, 'feature': feature, 'num_epochs':num_epochs, \n",
        "          'learning_rate':lr, 'optimizer':optimizer, 'dropout':dropout, \n",
        "         'es_min_delta':es_min_delta, 'es_patience': es_patience,\n",
        "          'random_state':random_state}\n",
        "        output_file.write(\"Model's parameters: {}\".format(training_parameters))\n",
        "\n",
        "        # For recording best results\n",
        "        best_loss = 1e5\n",
        "        best_epoch = 0\n",
        "        best_accuracy = 0\n",
        "        best_confusion_matrix = []\n",
        "\n",
        "        # For recording samples / sec\n",
        "        epoch_throughput = 0\n",
        "        epoch_duration = 0\n",
        "        mean_test_accuracy = 0\n",
        "\n",
        "        with tf.Session(config=session_conf) as sess:\n",
        "            sess.run(init)\n",
        "            for epoch in range(num_epochs):\n",
        "                sess.run(train_iterator.initializer)\n",
        "                sess.run(test_iterator.initializer)\n",
        "                train_handle = sess.run(train_iterator.string_handle())\n",
        "                test_handle = sess.run(test_iterator.string_handle())\n",
        "                train_iter = 0\n",
        "\n",
        "                # Record time\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Keep track of training loss\n",
        "                train_loss_ls = []\n",
        "                num_samples = 0\n",
        "\n",
        "                while True:\n",
        "                    try:\n",
        "                        # Perform an iteration\n",
        "                        _, tr_loss, tr_accuracy, step, samples = sess.run(\n",
        "                            [train_op, loss, accuracy, global_step, batch_size],\n",
        "                            feed_dict={handle: train_handle, keep_prob: dropout})\n",
        "                        \n",
        "                        # Save loss for plotting\n",
        "                        iteration_history['training_loss'].append(tr_loss)\n",
        "\n",
        "                        # Plot loss history at regular intervals\n",
        "                        if train_iter % plot_interval == 0:\n",
        "                            live_plot_double(iteration_history, epoch_history)\n",
        "                            print(\"Epoch: {}/{}, Epoch duration/sec: {}, Samples/sec: {}, Test accuracy: {}\".format(epoch + 1, num_epochs,\n",
        "                                                                                    epoch_duration, epoch_throughput, mean_test_accuracy))\n",
        "\n",
        "                        # Record this epoch's losses so we can get mean later\n",
        "                        num_samples += samples\n",
        "                        train_loss_ls.append(tr_loss * samples)\n",
        "              \n",
        "                        train_iter += 1\n",
        "                    except (tf.errors.OutOfRangeError, StopIteration):\n",
        "                        break\n",
        "\n",
        "                # Record duration of epoch\n",
        "                epoch_duration = time.time() - start_time\n",
        "                epoch_throughput = num_samples / epoch_duration\n",
        "                \n",
        "                # Plot train loss\n",
        "                mean_train_loss = sum(train_loss_ls) / num_samples\n",
        "                epoch_history['train_loss'].append(mean_train_loss)\n",
        "\n",
        "                # Calculate the test loss\n",
        "                if epoch % test_interval == 0:\n",
        "                    loss_ls = []\n",
        "                    accuracy_ls = []\n",
        "                    confusion_matrix = np.zeros([num_classes, num_classes], np.int32)\n",
        "                    num_samples = 0\n",
        "                    while True:\n",
        "                        try:\n",
        "                            test_loss, test_accuracy, test_confusion, samples = sess.run(\n",
        "                                [loss, accuracy, confusion, batch_size],\n",
        "                                feed_dict={handle: test_handle, keep_prob: 1.0})\n",
        "                            loss_ls.append(test_loss * samples)\n",
        "                            accuracy_ls.append(test_accuracy * samples)\n",
        "                            confusion_matrix += test_confusion\n",
        "                            num_samples += samples\n",
        "                        except (tf.errors.OutOfRangeError, StopIteration):\n",
        "                            break\n",
        "\n",
        "                    # Get test loss\n",
        "                    mean_test_loss = sum(loss_ls) / num_samples\n",
        "\n",
        "                    mean_test_accuracy = sum(accuracy_ls) / num_samples\n",
        "\n",
        "                    output_file.write(\n",
        "                        \"Epoch: {}/{} \\nTest loss: {} Test accuracy: {} \\nTest confusion matrix: \\n{}\\n\\n\".format(\n",
        "                            epoch + 1, num_epochs,\n",
        "                            mean_test_loss,\n",
        "                            mean_test_accuracy,\n",
        "                            confusion_matrix))\n",
        "                    \n",
        "                    # Update loss plot\n",
        "                    epoch_history['test_loss'].append(mean_test_loss)\n",
        "                    epoch_history['test_accuracy'].append(mean_test_accuracy)\n",
        "                    live_plot_double(iteration_history, epoch_history)\n",
        "                    print(\"Epoch: {}/{}, Epoch duration/sec: {}, Samples/sec: {}, Test accuracy: {}\".format(epoch + 1, num_epochs,\n",
        "                                                                                    epoch_duration, epoch_throughput, mean_test_accuracy))\n",
        "\n",
        "                    # Keep track of best test loss and save it to disk\n",
        "                    if mean_test_loss + es_min_delta < best_loss:\n",
        "                        best_loss = mean_test_loss\n",
        "                        best_accuracy = mean_test_accuracy\n",
        "                        best_confusion_matrix = confusion_matrix\n",
        "                        best_epoch = epoch\n",
        "                        saver.save(sess, save_path + os.sep + \"char_level_cnn\")\n",
        "                    # Stop training if test loss is no longer decreasing\n",
        "                    if epoch - best_epoch > es_patience > 0:\n",
        "                        print(\"Stop training at epoch {}. The lowest loss achieved is {}. The corresponding accuracy achieved is {}.\".format(epoch, best_loss, best_accuracy))\n",
        "                        break\n",
        "        \n",
        "        # Print vital statistics\n",
        "        print('')\n",
        "        print(\"Final five test losses:\")\n",
        "        print('\\n'.join('{}: {:.4f}'.format(*k) for k in enumerate(epoch_history['test_loss'][-5:])))\n",
        "        print('')\n",
        "        print(\"Best test loss: {:.4f}\".format(best_loss))\n",
        "        print(\"Corresponding test accuracy: {:.4f}\".format(best_accuracy))\n",
        "        print(\"Corresponding confusion matrix: \\n{}\\n\\n\".format(best_confusion_matrix))\n",
        "\n",
        "        output_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FpLI29xqDi5",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuCDaclyo_a6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example for classifying DNA sequences\n",
        "train(training_path='/path/to/train.tsv', test_path='/path/to/test.tsv', \n",
        "      save_path='/path/to/savedir', feature='small', lr=1e-4, \n",
        "      batch_size=128, alphabet=\"GATC\", random_state=42, num_epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}